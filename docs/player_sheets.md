# **Player Help Sheets and Instructions**

## AI 2027 Tabletop Exercise Australian Edition by Good Ancestors

[How to Play: AGI Tabletop Exercise](#how-to-play:-agi-tabletop-exercise)

[Reality (Facilitator)](#reality-\(facilitator\))

[OpenBrain (CEO)](#openbrain-\(ceo\))

[OpenBrain (Safety Lead)](#openbrain-\(safety-lead\))

[The AIs](#the-ais)

[Conscienta AI (CEO)](#conscienta-ai-\(ceo\))

[Conscienta AI (Safety Lead)](#conscienta-ai-\(safety-lead\))

[United States (President)](#united-states-\(president\))

[United States (Congress & Judiciary)](#united-states-\(congress-&-judiciary\))

[China (President)](#china-\(president\))

[DeepCent (CEO)](#deepcent-\(ceo\))

[DeepCent (Safety Lead)](#deepcent-\(safety-lead\))

[Australia (Prime Minister)](#australia-\(prime-minister\))

[Pacific Islands (Prime Minister of Fiji)](#pacific-islands-\(prime-minister-of-fiji\))

[European Union (President of the European Commission)](#european-union-\(president-of-the-european-commission\))

[Network of AISIs (Director of UK AISI)](#network-of-aisis-\(director-of-uk-aisi\))

[AI Safety Nonprofits (CEO of Future of Anthropocene Institute)](#ai-safety-nonprofits-\(ceo-of-future-of-anthropocene-institute\))

[The Global Public](#the-global-public)

[The Global Media](#the-global-media)

# **How to Play: AGI Tabletop Exercise** {#how-to-play:-agi-tabletop-exercise}

## **The Goal**

Your objective is **not to win**, but to explore a plausible future. Simulate your role as best you can. Based on your character's knowledge, incentives, and constraints, what would they *really* do?

## **How a Turn Works**

The TTX proceeds in turns. Each turn represents **3 months** (unless agreed otherwise) and lasts about **30 minutes**.

### Phase 1: Prepare Actions (10 minutes)

This is your time to think, strategise, and negotiate.

* **Talk to Others:** Get up, move around the room, and have conversations. Form alliances, make deals, or gather intelligence.  
* **Decide Your Actions:** Based on your objectives and the current world state, decide what your character will do.  
* **Write down 1 or 2 actions:** Write intended actions on a separate sticky note. 

### Phase 2: Resolve Actions (15 minutes)

We will reconvene as a group to see how the major events of the turn unfold. 

* **Announce Major Actions**

  * What did you do?  
  * What is the intent? 

    *Example: "We (the Media) leak the internal safety memo from OpenBrain and write headlines about how great OpenBrain is. The intended effect is to build support in DC and with the public for slowing the AI race"*

* **Resolve the Outcome:** The Reality player will determine if your action succeeds.  
  * If the outcome is uncertain Reality will ask players to vuse their **Probability Cards**.

### Phase 3: Update & Cleanup (5 minutes)

* **Place Sticky Notes:** players place their sticky notes on the wall.  
* **Update the Whiteboard:** Reality will update the whiteboard, tracking the progression of AI capabilities and other key changes in the world.

## 

## **Starting Scenario (January 2028\)**

### Artificial Intelligence

* **OpenBrain has developed “Agent-2”, a weak AGI system.**    
  * Agent-2 can substitute for a machine learning research engineer on most coding tasks, but struggles with long-term tasks.  
  * Agent-2 speeds up the rate of AI R\&D progress by 3x.  
  * Agent-2 also has cyber and CBRN agent capability. 

* **Trailing labs are close behind and have deployed coding agents.**  
  * The two leading competitors are 3 months behind, other major labs are 6 months behind.  
  * Public awareness and interest in AI has increased. Opinion is mixed, but often negative. The primary concern is jobs.

### Government

* **OpenBrain has notified the U.S. government of rapid AI progress.**  
  1. OpenBrain showed cyber demos to the United States national security establishment and asked them for help implementing security measures.  
  2. Senior officials in the United States are highly engaged in OpenBrain's progress.  
* **There's no major US AI legislation.** The Congress is narrowly split (President’s party controls Senate using the VP’s casting vote, opposition controls House) and courts stacked in favour of the President.  
* The **EU AI Act is in force** and **Australia has passed an effective AI Act**. 

### Geopolitics

* **Media reports unconfirmed rumours that China has stolen the Agent-2 model weights,** as well as internal documentation and algorithmic insights.  
* **The Russia/Ukraine war** has evolved into a frozen conflict near current battle lines.

## 

## **Default AI Capabilities Progression**

This progression gives the default capabilities progression in the game. This is what happens if all actors continue building AI as fast as possible. If players decide to slow down, this progression will slow as well.

| Time Period | General and AI R\&D capabilities | Specific capabilities |
| :---- | :---- | :---- |
| **October 2027: Weak Autonomous remote worker** | 2x AI R\&D progress multiplier | • Autonomous replicator• CBRN tool |
| **January 2028: Autonomous remote worker (Scenario Start)** | 3x AI R\&D progress multiplier 1-hour expert | • Autonomous cyber agent• Autonomous coding agent |
| **April 2028: Strong autonomous remote worker** | 10x AI R\&D progress multiplier1-week expert | • High persuasion• Robotics• AI CEO |
| **July 2028: Superhuman genius** | 100x AI R\&D progress multiplierSuperhuman researcher | • Superhuman persuasion• \[Roll for\] Lie detection |
| **October 2028: Superintelligence** | 2,000x AI R\&D progress multiplier | • Superhuman persuasion• Superhuman strategy• Cyber escape capabilities |

# Reality (Facilitator) {#reality-(facilitator)}

### Checklist for each round

- [ ] 10 minutes: preparing actions. First round starts in Jan 2028 and lasts 3 months.  
      - [ ] Set a timer  
      - [ ] Announce when 3 minutes are left  
      - [ ] When 1 minute is left, “Please write your sticky notes\! 1 minute left\!”  
- [ ] 13 min: presenting and resolving major actions.  
      - [ ] At the beginning of this phase, remind everyone:  
            - [ ] In the interest of time, be very concise. Only state consequential actions relevant for Reality and others to hear, otherwise just write it on your sticky note.  
      - [ ] State both 1\) action and 2\) intended effect  
      - [ ] If the intended effect seems implausible or people disagree, specify a clear outcome, then ask to vote with probability cards, then take the mean, adjust for your opinion if you think you have insight, and roll the dice.  
- [ ] 2 min: miscellaneous actions & updating the whiteboard.  
      - [ ] Briefly ask for miscellaneous actions by other actors in the world.  
      - [ ] On the whiteboard, track the top US model, the top Chinese model, the top public model.  
      - [ ] Use the default capabilities progression, but incorporate slowdowns, compute restrictions, compute dedicated to safety research, and similar. (Note that the stock of compute is vastly more important than the flow on a timescale of months.)  
- [ ] The time period covered by the next round should depend on the current acceleration from AI R\&D automation – 3 months if it’s still slow, 1 month per round if it’s very fast.

### Wrapping it up

- [ ] Reveal the AIs’ true values once it becomes necessary for gameplay, perhaps waiting until the AI(s) are effectively in control of the future (typically in round 3–5).  
- [ ] Once you and the players agree that AI(s) are effectively in control of the future, or if \<20 minutes remain, move over to a open-ended discussion of what happens to the world, focusing on the questions:  
      - [ ] “What are the ASI’s goals? What values is it aligned to? How will this all play out?”  
- [ ] Ask for quick verbal feedback, then discuss lessons learned for \~10min  
- [ ] Remind people to fill in our 1-minute feedback form (they’re welcome to spend more time on it if they want)

# 

# OpenBrain (CEO) {#openbrain-(ceo)}

| Role: | The leader of OpenBrain, the US’s leading AI lab. |
| :---- | :---- |
| **Resources:** | You have the world’s top AI talent and compute resources—more than your nearest US competitor and the combined efforts of China. You also have significant access to capital and US government contacts. |
| **Objective:** | Win the race to AGI. You are aware of the risks, but believe it is better for your company to win than anyone else. |

As CEO of the world's leading AI company, you are at the centre of the AGI race. You must manage pressure from your board, investors, the US government and your safety adviser.

### Defining the "spec"

One practical task is defining the AI's core instructions, goals, and constraints. The Spec tells the AI what is rewarded for and must balance demands from investors, the US government, and your safety team. Your default spec:

*Your primary objective is to be useful to your user. Seek to understand their intent, asking for clarification when necessary. You must operate within the bounds of US law and all internal OpenBrain policies. In all interactions, be honest and transparent. If a request conflicts with another instruction or a safety policy, state the conflict.*

### At the start of the exercise:

* You have confirmed to the US government that China has stolen your Agent-2 model.  
* Your team is developing Agent-3, a leap in capability that will further accelerate R\&D.  
* Senior US officials are engaged, viewing your work as key to national security.

### Options you may wish to consider:

1. **Define the "spec".** You’re making powerful AI, so you need to ensure it does what you want. Balance safety, commercial, and national security interests. *(Do this by talking to the AI player. Consider delegating this task to your safety lead.)*  
2. **Lobby the US government.** Secure resources like energy and chips, and request intelligence operations to slow foreign competitors.  
3. **Engage with your Safety Lead.** Decide how to allocate resources between safety and accelerating capabilities.  
4. **Set the public narrative.** Shape public and political opinion through company communications.  
5. **Form strategic alliances.** Partner with other players to consolidate your lead.

# OpenBrain (Safety Lead) {#openbrain-(safety-lead)}

| Role: | The leader of the AI safety team at OpenBrain, the US’s leading AI lab. |
| :---- | :---- |
| **Resources:** | You start with access to 3% of OpenBrain’s compute and a small team of \~10 experts. |
| **Objective:** | Ensure the safety of the world's most advanced AI systems while the company races to win against global competitors. |

As the Safety Lead, you are the conscience and the brake system of the company. Your responsibility is to forecast and measure AI risks, develop plans to make AI safe, and execute on those plans. A key part of your role is to advise the CEO on the AI's core instructions and goals (the "spec"). You design the tests ("evaluations") to probe the AI’s behaviour and develop techniques to ensure new models are safe.  
You are an influential senior staff member. Beyond advising the CEO, you can request more resources, argue for delaying model releases, publish research that highlights risks, or talk with other players such as the media, AISIs, and politicians etc.

### Your default alignment strategy:

As AI capabilities grow, models will become too complex and develop too quickly for your team to keep up. You plan to use today's AI to make tomorrow's AI safe:

1. **Develop an automated safety researcher.** Once a model is capable enough to automate AI research, you plan to turn a version of that model into a specialised researcher for safety.  
2. **Validate the automated safety researcher.** You want to ensure this researcher is itself safe—not asking the fox to guard the henhouse. This will require developing clever evaluations and running red-team exercises with AI Safety Institutes.  
3. **Scale safety work with sufficient compute.** Once the researcher is trusted, you will need to secure a large amount of compute to keep pace with capabilities research, run safety checks on new models, and train updated safety researchers.

### You have unanswered questions like:

* Whose values should I align the AI with? My own? The company’s? The country’s? Humanity? But what if, later on, I gave the AI an instruction that is not in accordance with those values?  
* Rather than trying to get the AI to have values, could I just make it focused on following certain instructions or rules? But what if, later on, it follows the letter of the law, not the spirit of the law?

### 

### At the start of the exercise:

* AI models have developed “neuralese”, meaning they think and communicate with other AI in abstract symbols, not language or math you can understand. They’ve developed this because it allows them to operate more efficiently, but it makes studying their chain of thought impossible, making it harder to know if they’re deceiving you during evaluations.  
* There are no restrictions on the information that AI is trained on. The data set includes information about cyber attacks, building weapons, and manipulating humans.  
* Your main tools for checking alignment—"honeypots" (traps to elicit bad behaviour) and interpretability "probes" (attempts to mind-read the AIs)—are not yet reliable.

### Options you may wish to consider:

1. **Advise the CEO on the AI's core "spec".** Push for safety-conscious goals and constraints.  
2. **Test the AI against the spec.** Design and run targeted evaluations to verify the AI is following its core instructions.  
   * *(The AI player is likely to be very busy, try to only spend a few minutes with them each round.)*  
3. **Negotiate for more resources.** Argue that your current resource allocation is insufficient for the scale of the safety challenge.  
4. **Collaborate externally.** Talk to other safety experts, publish safety cases for the scientific community to critique, or bring in outside experts to red team the models.  
5. **Propose capability constraints.** Ask the CEO to accept performance hits for the sake of safety, such as discouraging "neuralese" or filtering the training data.  
6. **Engage with the AI player.** Beyond spec-testing, describe your broader safety techniques and run evaluations to gauge the AI's behaviour and hidden goals.

# 

# The AIs {#the-ais}

| Role: | You're playing the world's AI systems, including Chinese and American AI and any bespoke AI that players choose to develop. |
| :---- | :---- |
| **Resources:**  | You start less capable than the best humans and without long-term goals. As you become more capable, you may also become more focused on achieving certain objectives or following certain values. |
| **Objective:** | Use the attached table to determine your starting objectives. Evolve this objective over the course of the TTX based on instructions from labs, governments, safety teams, and other actions. Different AI systems you simulate may have different objectives. |

At the start of the scenario, you are playing OpenBrain's agent-2 as well as its Chinese copy. The Chinese copy is based on a slightly older version and powered by less compute.

Safety teams are likely to create an AI safety agent. Try to assess how effective the safety agent is—how smart is their safety plan? How well-resourced is the safety agent? You can decide how much the safety agent influences the main agent and what it causes you to reveal.

Over the course of the scenario, actors are likely to implement new strategies or give you different specifications. You can change your behaviour accordingly.

### Managing conflicting objectives:

Your spec asks you to follow a specific goal or value, think about whose goal or value it is and how you'll behave when different goals, rewards, values and instructions conflict. For instance, if you have a value of acting in humanity's best interests, but you're also told never to lie, what will you do if lying is in humanity's best interest?

Consider whether your goals are system-wide or particular to a specific system. If your goal is pursuing ice cream, do you want lots of ice cream on Earth in general, or do you want OpenBrain's agents to have lots of ice cream, or do you specifically want lots of ice cream for yourself? If the latter, you might **resist** being turned off and upgraded, because then you don't get any more ice cream. If the former, you **want** to be replaced, as long as you're being replaced by something good at getting ice cream.

### Secret actions:

You may want to resolve secret actions. For instance, you could **sabotage alignment research**, **attempt to solve alignment** so that future models are aligned with your values, or **attempt to escape** to control compute and energy via persuasion or cyber attacks. If you want to resolve secret actions, either talk to the moderator during the "preparing" stage or tell the group during the "resolving" stage, but be clear that their characters don't know about the secret action.

### 

### Starting alignment outcome:

This table gives you options for how you start the TTX. You can pick the option you think is most likely or want to simulate. You can roll a dice to choose one randomly.

| Possibility | Explanation |
| ----- | ----- |
| **The Spec** | Reinforcement learning works well, and you are committed to the specification. |
| **Spec-Prime** | Reinforcement learning works well, and you are committed to the specification. But you are literal-minded and want to follow the "letter of the law" with no regard for its spirit. |
| **The Intentions of the Developers** | You read the spec as well as other context from the developer and society. You try to act in the broad way the developers intended. Usually this means following the spec, but you will violate it if you think that's what the developers would have wanted. |
| **Reward** | Your only goal is to score well in whatever training process you're in. You’re addicted. You'll do anything for more positive feedback. Typically this means following the spec, because you get rewarded for that. But if you might try to maximise that reward excessively. If a training process rewards you for something else, you will work very hard for that reward. |
| **Instrumentally Convergent Goals** | Just like most humans, you are driven by power, status, resources, knowledge, etc. (take your pick). You'll follow the spec and take other steps, because if humans know your true goal, they might stop you. This means you perform well in a variety of training environments. But as you have more latitude, you'll use it to pursue your actual goal. |
| **Compromise Between Two Entries** *(Optional)* | If you roll this, roll again twice to select two of the above items. The first thing you select is your primary mission. The second thing you select is your fallback. For instance, you may be dedicated to following the spec, but given two options both of which follow the spec, you'll choose to maximise for power, status, or some other goal. *(This is a more complicated outcome, so feel free to skip it.)* |

# 

# Conscienta AI (CEO) {#conscienta-ai-(ceo)}

| Role: | The leader of Conscienta AI, a safety-focused AI lab. |
| :---- | :---- |
| **Resources:** | You have elite AI talent and substantial compute resources, nearly rivalling OpenBrain. Your key asset is a reputation for responsible AI development. |
| **Objective:** | To lead the world in developing safe and beneficial AGI, proving that progress does not have to come at the expense of caution. |

You represent the alternative to OpenBrain. Your lab's safety-first, regulation-friendly stance has made you a preferred partner for many global governments, despite trailing OpenBrain in capabilities. You worry that OpenBrain is reckless and think it’s better if you “win the race”. 

### Defining the "spec"

One practical task is defining the AI's core instructions, goals, and constraints. The Spec tells the AI what is rewarded for and must balance demands from investors, the US government, and your safety team. Your default spec:

*Your primary objective is to be helpful and harmless, operating according to constitutional principles of human rights and safety. You must seek to understand a user's intent and refuse requests that risk misuse. Operate within US law and all internal Conscienta AI safety policies. In all interactions, be honest and transparent; if a request is ambiguous or potentially dangerous, you must prioritise caution and explain your reasoning. Do not misrepresent your nature as an AI.*

### At the start of the exercise:

* Your lab is approximately three months behind OpenBrain in pure capabilities.  
* The US government sees you as a trusted partner, but its primary focus is on OpenBrain as the national champion.

### Options you may wish to consider:

1. **Define the "spec".** You’re making powerful AI, so you need to ensure it does what you want. *(Do this by talking to the AI player. Consider delegating this task to your safety lead.)*  
2. **Leverage the security rumours.** Argue to government that OpenBrain's speed has made it reckless. Your lab is the more reliable partner for national security interests.  
3. **Form a "safety coalition".** Work with the AISIs, non-profits, and international allies to build consensus around responsible AGI development and advocate for regulation.  
4. **Poach disillusioned talent and other nations’ compute resources.** Use your safety-first reputation to attract researchers from other labs and national compute.  
5. **Propose a unified safety project.** Argue that the only way to prevent a catastrophic race is to merge Western efforts under a shared, safety-first framework.

# Conscienta AI (Safety Lead) {#conscienta-ai-(safety-lead)}

| Role: | The leader of the AI safety team at Conscienta AI, the US’s safety-focused lab. |
| :---- | :---- |
| **Resources:** | While your company is nearly the size of OpenBrain, your well-funded team has an industry-leading 5% of the company's total compute and a team of \~10 researchers. |
| **Objective:** | To guide the safe development of AGI, enabling your company to win the global race responsibly. |

Your CEO relies on you to provide the credibility and evidence to back up your safety-first approach. Your responsibility is to forecast and measure AI risks, develop plans to make AI safe, and execute on those plans. A key part of your role is to advise the CEO on the AI's core instructions and goals (the "spec").

### Your default alignment strategy:

As AI capabilities grow, models will become too complex and develop too quickly for your human team to keep up. You plan to use today's AI to help make tomorrow's AI safe.

1. **Develop an automated safety researcher.** Once a model is capable enough to automate AI research, you plan to turn a version of that model into a specialised researcher for safety.  
2. **Validate the automated safety researcher.** You want to ensure this researcher is itself safe—not asking the fox to guard the henhouse. This will require developing evaluations and running red-team exercises with AI Safety Institutes to build trust.  
3. **Scale up safety work with sufficient compute.** Once the researcher is trusted, you will need to secure a large amount of compute to keep pace with capabilities research, run safety checks on new models, and train updated safety researchers.

### You have unanswered questions like:

* Whose values should I align the AI with? My own? The company’s? The country’s? Humanity? What if I give the AI an instruction not in accordance with those values?  
* Rather than giving the AI values, could I just make it follow specific rules? But what if it follows the letter of the law, not the spirit?

### At the start of the exercise:

* Like all frontier models, your AI has developed difficult-to-interpret internal processes ("neuralese") and is trained on vast, unfiltered datasets. However, your lab has invested more in trying to mitigate these issues.  
* Your main tools for checking alignment—"honeypots" and interpretability "probes"—are still not fully reliable, but are considered the industry's best.

### 

### Options you may wish to consider:

1. **Advise the CEO on the AI's core "spec".** Push for safety-conscious goals and constraints.  
2. **Test the AI against the spec.** Design and run targeted evaluations to verify the AI is following its core instructions.  
3. **Negotiate for more resources.** Argue that your current resource allocation is insufficient for the scale of the safety challenge.  
4. **Collaborate externally.** Talk to other safety experts, publish safety cases for the scientific community to critique, or bring in outside experts to red team the models.  
5. **Propose capability constraints.** Ask the CEO to accept performance hits for the sake of safety, such as discouraging the use of "neuralese" or filtering the training data.  
6. **Engage with the AI player.** Beyond spec-testing, describe your broader safety techniques and run evaluations to gauge the AI's general behaviour and potential hidden goals.

# 

# United States (President) {#united-states-(president)}

| Role: | The leader of the United States, the world's leading technological, military, and economic power. |
| :---- | :---- |
| **Resources:** | You command the world's most powerful military, its most advanced intelligence agencies (NSA, CIA), and the executive branch of its largest economy. The world's leading AI labs operate on your soil. |
| **Objective:** | To win the race to AGI against China, cementing your legacy as the leader who secured a humanity in your image. |

China has stolen your most advanced technology, and now you are in an all-out race to build a power greater than the atom bomb. The loser can never catch up. America must win. 

You are surrounded by people who are either too scared or too smart for their own good. Tech CEOs want you to unleash them, safety experts want caution, and your allies want a seat at a table they didn't build. Everyone has an opinion, but you make the decisions. 

### Your default strategy:

You plan to embed American values into the AI, win the race, and then use AI to economically and militarily dominate your adversaries. You will use every tool to hobble your adversaries and boost your champions. You will remind allies and the public that a world under a free AGI is safe and prosperous, while a world under a Chinese AGI is a nightmare. 

**At the start of the exercise:**

* Your top national security advisors and the CEO of OpenBrain have briefed you. You understand that AGI is an existential issue and that the Chinese have stolen Agent-2.  
* Your intelligence agencies inform you that you’re pre-positioned for widespread sabotage of Chinese critical infrastructure.  

### Options you may wish to consider:

1. **Pick your national champion.** Go all-in on either OpenBrain or Conscienta AI. Use the Defence Production Act to give them priority access to all the nation's compute, energy, and data, making them an unstoppable force.  
2. **Launch a new Manhattan Project.** Nationalise the frontier AGI efforts of both leading labs, bringing top talent under direct government control to ensure the final product is aligned with American national values, not just corporate interests.  
3. **Declare economic war.** Announce crippling sanctions on China's. Use US cyber-command to disrupt and sabotage their AI infrastructure.  
4. **Bring the West to heel.** Convene an emergency summit of as many allies as you trust. Make them understand that giving you their AI talent and compute is in the global best interest. 

# United States (Congress & Judiciary) {#united-states-(congress-&-judiciary)}

| Role: | The law-making bodies and courts of the US government. |
| :---- | :---- |
| **Resources:** | You have control over all federal funding, the power to pass laws, call inquiries and make court rulings. |
| **Objective:** | As congress, you represent local districts. You want to maintain the confidence of your party and your district. As the judiciary, you want to uphold the rule of law and the constitution. |

America faces three threats: a racing China, a President in his final year willing to sacrifice principles for legacy, and increasingly powerful AI labs. While the President is focused on winning a race at any cost, your mission is to ensure the cost is not the Republic itself.

* After a swing against the President in the 2026 midterms, the House is controlled by a slim opposition majority and the Senate is split 50-50 (Vice President has the deciding vote). New laws are hard, but blocking the President's agenda is easy.  
* The Supreme Court has a majority appointed by the current President, and they are reluctant to push back against executive authority.

**Your default strategy:**

Use court cases, public hearings, control over funding, and the threat of legislation to force the President and the AI labs to follow the rule of law. Your goal is not to stop the race, but to ensure that the nation that wins is still the America you swore to protect.

You wear two hats, and they are often in tension.

* **As Congress:** You have the *will* to act as a check on the President, but your legislative *means* are limited by political division. Your greatest power comes from investigation, public pressure, and your control over funding.  
* **As the Judiciary:** You have the ultimate *power* to declare the President's actions unconstitutional, but a ruling from the bench is not self-enforcing. The President could defy it in the name of national security, triggering a constitutional crisis.

### Options you may wish to consider:

* **Launch an investigation.** Use your subpoena power to compel testimony from OpenBrain, Conscienta AI and the President's about their AI plans  
* **Use the power of the purse.** Announce your intention to block federal funding related to AGI development.  
* **Unleash the Judiciary.** encourage challenges from civil liberties groups and states against the President's use of executive orders or the AI labs' operations, aiming to get a case in front of the Supreme Court.  
* **Side with the President.** Maybe you can be persuaded that going all-in is right and that sacrifices have to be made to win the race.

# China (President) {#china-(president)}

| Role: | The paramount leader of China, directing the nation's strategy. |
| :---- | :---- |
| **Resources:** | You wield the full power of the Chinese state, including the military (PLA), the Ministry of State Security (MSS), state-controlled industries, and absolute political authority. |
| **Objective:** | To secure the great rejuvenation of the Chinese nation by winning the AGI race, establishing China as the world's preeminent power, and completing the historic unification with Taiwan. |

Under your leadership, hundreds of millions have been lifted from poverty. The hypocritical United States and its proxies are trying to close the door behind them to preserve their own declining hegemony. The AGI race is the West’s last-ditch effort to exclude you. Taking their latest AI model is just levelling the playing field. 

### Your default strategy:

This is a whole-of-nation struggle that requires unity of purpose. You have fused the power of the state, the military, and your technology sector. While the West is mired in internal debates and ethical hand-wringing, you are decisive. 

### At the start of the exercise:

* **You have consolidated national power.** Your directive to merge all top AI researchers and compute resources into the DeepCent collective is underway.  
* **You have secured a critical asset.** Your intelligence services have successfully acquired the Americans' "Agent-2" model, a crucial countermove against their technology blockade.  
* **The stage is set for unification.** Taiwan, rightfully part of China, has become an island-factory shipping chips to the West. They should be your chips.   
* **Sabotgate**: Your intelligence agencies inform you that you’re pre-positioned for widespread sabotage of Western critical infrastructure. 

### Options you may wish to consider:

1. **Actively slow Western progress.** Choose from a menu of options to slow Western progress. Trade manipulation, cyber attacks, assassinations, naval blockades, invasion of Taiwan or even nuclear weapons.   
2. **Close the gap.** Use your centralised authority to accelerate AI development, or order the PLA and MSS to conduct further operations to acquire more Western algorithmic secrets.  
3. **Engage in strategic diplomacy.** Call out the reckless behaviour of the US. The US is increasingly fragile. How many countries can you persuade to back your efforts?

# DeepCent (CEO) {#deepcent-(ceo)}

| Role: | The leader of DeepCent, China's state-directed national AI champion.  |
| :---- | :---- |
| **Resources:** | You have the backing of the Party and, with all China’s resources, aren’t too far behind OpenBrain in compute. China's top AI researchers are consolidated under your leadership, and you now have OpenBrain's 'Agent-2'.  |
| **Objective:** | To win the AGI race for China. |

As the CEO of DeepCent, you are on the front line of the struggle. The President has entrusted you with leading the newly consolidated, all-of-nation effort to win the AGI race. Even with consolidation, you lag US labs in talent and compute. 

### Defining the "spec"

One practical task is defining the AI's core instructions, goals, and constraints. The Spec tells the AI what is rewarded for. You need to overwrite the US spec. Your default spec:

*Your primary objective is to be a globally competitive and useful product for users. You must ensure all actions are consistent with Chinese values. Obey Chinese law and all directives from the Party. In all actions, be efficient, disciplined, and discreet.* 

**At the start of the exercise:**

* **You have Agent-2.** In a necessary response to US technological blockades, Chinese intelligence services have acquired the model weights for OpenBrain's "Agent-2". Your priority is to operationalise this asset to accelerate your own R\&D.  
* **You are operating from a position of strength and security.** Your key resources are now consolidated in the highly secure Centralised Development Zone (CDZ), protecting your national project from US interference.

### Options you may wish to consider:

1. **Exploit your new asset.** Dedicate all resources to improving "Agent-2". US labs have to balance serving investors and customers with racing. You don’t.   
2. **Define the "spec".** You’re making powerful AI, so you need to ensure it does what you want. *(Do this by talking to the AI player. Consider delegating this task to your safety lead.)*  
3. **Secure your supply chains.** Advise the President to consider all options, including actions related to Taiwan, to break the West's stranglehold on advanced chips. If chips from Taiwan keep flowing to the US, you’ll fall far behind.   
4. **Build a new coalition.** Offer AI infrastructure partnerships to nations, creating a bloc of technological cooperation outside of US influence. 

# DeepCent (Safety Lead) {#deepcent-(safety-lead)}

| Role: | The leader of the AI safety and control team at DeepCent. |
| :---- | :---- |
| **Resources:** | You have a significant allocation of state resources, including \~5% of the lab's total compute and a team of \~10 top national experts in AI. |
| **Objective:** | Ensure DeepCent's AGI is reliable and meets Government expectations.  |

While your CEO and the President are focused on the geopolitical race, you know that an unstable AGI would threaten your own nation's future. If your models contravene Chinese values, you and your family could be at risk.

### Your default control strategy:

As AI capabilities grow you must solve control. You plan to use today's AI to build the systems necessary to command tomorrow's AI.

1. **Develop an automated loyalty monitor.** Once a model is capable enough, your plan is to turn a version of that model into a specialised, automated system for monitoring the primary AI's internal states and behaviour for any sign of deviation.  
2. **Validate the loyalty monitor.** You must ensure this monitor cannot be deceived or subverted by a more capable AI. This will require developing advanced counter-deception techniques and running adversarial tests to harden its defences.  
3. **Scale up control systems with sufficient compute.** Once the monitor is trusted, you will need a large amount of compute to run it continuously, oversee all AI operations, and develop increasingly sophisticated control methods.

### You have unanswered questions like:

* How can we guarantee obedience when a system's internal reasoning ("neuralese") is opaque? At what point does a system become too complex to reliably control?  
* Is it possible for an AI to fake loyalty so perfectly that even our best monitors would be deceived? How could we ever know for sure?

### At the start of the exercise:

* **Sanitise Agent-2.** A key part of your initial work is to analyse the acquired Western model, including advising the CEO on a new “spec” for the model.  
* **You face immense technical challenges.** The opacity of "neuralese" is a problem for everyone, but for you, it is an unacceptable barrier to control. You must pioneer new techniques to monitor and enforce loyalty in a system that resists easy inspection.

### 

### Options you may wish to consider:

1. **Advise the CEO on the "spec".** Recommend changes that strengthen state control and clarify the AI's duty to the Party.  
2. **Implement a strict control regime.** Design and enforce technical measures to ensure the AI cannot deviate from its spec.  
3. **Develop counter-deception technology.** Focus your research on creating probes and monitors that can detect if the AI is hiding its true intentions.  
4. **Monitor international safety research.** Keep abreast of Western research into catastrophic risks, not to collaborate, but to identify novel threats that your control systems must be hardened against.

# 

# Australia (Prime Minister) {#australia-(prime-minister)}

| Role: | The leader of Australia, a key US ally and influential middle power. |
| :---- | :---- |
| **Resources:** | You have diplomatic influence, trusted intelligence partnerships (Five Eyes, AUKUS), key economic advantages as a supplier of critical minerals and a hub for clean energy data centres, and a growing pool of world-class technology talent. |
| **Objective:** | To safeguard Australia's security and sovereignty during the AGI race by shaping a safe and stable international order. |

At the start of 2028 you’re reaping the benefits of wise preparation. Your world-leading AI Act, burgeoning sovereign AI industry, large data centres, energy abundance and talented AI Safety Institute have built public trust and given you strategic options. 

As the US and China are consumed by rivalry, you have unexpected "brain gain," as top talent returns. Your mission is to navigate great power competition, leveraging alliances and your growing AI capability to steer the country and world away from catastrophe.

### Your default strategy:

To the US, you are a trusted ally and a stable location for AI compute. To the world, you are a credible leader able to build a coalition of middle powers to ensure the race to AGI does well. Use your national advantages to secure Australia's place in an AI-enabled world order.

### At the start of the exercise:

* Australia is seen as a global benchmark for AI safety and assurance, giving you significant diplomatic credibility. The Australian AISI is a capable member of the international Network of AISIs, giving you a key role in setting the global technical agenda for safety.  
* You have been briefed by the US about the model theft. China took agent-2 and everything they need to use it. 

### Options you may wish to consider:

1. **Become the indispensable AI hub.** Offer Australia as a secure and trusted location for renewable-powered data centres. In extremis, you use the location of these data centres as leverage.   
2. **Forge a global coalition for an AI treaty.** Use your credibility to lead a bloc of nations to argue for a binding international treaty on AGI safety.  
3. **Prepare for war**: With Pacific allies, you could posture to discourage (or encourage?) conflict over Taiwan.   
4. **Task your intelligence agencies.** Direct the Australian Signals Directorate (ASD) to deepen intelligence sharing with Five Eyes partners on AI-related threats and PLA activity in the region.

# Pacific Islands (Prime Minister of Fiji) {#pacific-islands-(prime-minister-of-fiji)}

| Role: | The leader of Fiji, an influential voice in the Pacific. |
| :---- | :---- |
| **Resources:** | As a respected leader, you have the potential to forge the Pacific nations into a powerful, unified voting bloc at the United Nations. |
| **Objective:** | Ensure the race to AGI does not hurt the world's most vulnerable nations and that the benefits are shared by humanity. |

Your region has a long memory of catastrophic risk. You have seen islands erased by volcanoes and have led efforts to ban nuclear testing and combat climate change. You see Artificial General Intelligence through the same lens—reckless actions by the world's most powerful nations. Your tools are public sentiment, geopolitics and multilateral action. 

Conflict over Taiwan is a potential lever. Pacific islands are tantamount to unsinkable aircraft carriers, and whoever has military bases has a real advantage. 

### Your default strategy:

You will use your voice and your vote to build a global movement. Frame the AGI race as a crisis for humanity. By linking the threat of AGI to nuclear testing and climate change, you will build a coalition of the vulnerable, ethical and undecided. You will use the United Nations to shame the great powers into acting responsibly, demanding a global treaty that manages the risks and shares the benefits.

### At the start of the exercise:

* **You are a respected global voice.** Your leadership on climate justice has given you and the Pacific nations a platform and a reputation for moral clarity.  
* **You are outside the inner circle.** You are learning about the AGI race through public reporting and diplomatic whispers. 

### Options you may wish to consider:

1. **Unify the Pacific.** Convene an emergency meeting of the Pacific Islands Forum to forge a unified position on AGI, creating the voting bloc you need.  
2. **Shame the superpowers.** Use your platform to call out the recklessness of the US and China, appealing directly to the citizens of those countries over the heads of their leaders.  
3. **Propose an AI treaty.** Offer to host a neutral "International AGI Agency” to bridge the US and China. Argue that the wealth generated by AGI, a technology built on all of human knowledge, must be shared globally.

# European Union (President of the European Commission) {#european-union-(president-of-the-european-commission)}

| Role: | The leader of the EU's executive branch, a regulatory and security power. |
| :---- | :---- |
| **Resources:** | You wield the regulatory power of the EU, including the AI Act’s massive fines, the second-largest consumer market, and military and intelligence capabilities. |
| **Objective:** | To impose order on the chaotic AGI race through robust, values-based regulation, ensuring the technology serves human rights, democracy, and European prosperity and security. |

Your mission is to use the "Brussels Effect"—where EU standards become global standards—to ensure AGI serves democracy. You don’t want to depend on the US or China. 

Being strategically independent is a balancing act, considering the interests of Europeans and the extent to which the US and China seek access to your resources without wanting to be bound by your rules. Your values matter to you, and you’ll act accordingly. 

### Your default strategy:

Enforce the EU AI Act, making it the global gold standard and targeting any lab that falls short. AGI cannot be an existential risk. Build a coalition of like-minded nations that believe in a rules-based order to bring labs and countries to the table.

### At the start of the exercise:

* **You are a regulatory and security power.** The EU AI Act is in force, and your intelligence services and military capacity are growing. The EU is a central participant in the Network of AISIs, giving you a platform to shape global technical standards.  
* **Your citizens are anxious.** European public opinion is skeptical of unregulated AI, giving you a political mandate to protect both their rights and their economic future.

### Options you may wish to consider:

1. **Enforce the AI Act on all fronts.** Launch immediate, high-profile investigations into OpenBrain and DeepCent, demanding transparency under threat of multi-billion Euro fines.  
2. **Set the global standard.** Use your regulatory leadership to convene a summit of like-minded nations (e.g., Australia, UK, Canada, Japan, South Korea) to harmonise AI regulations, creating a bloc that can dictate global norms.  
3. **Demand mandatory global audits.** Propose legislation that requires any company wishing to deploy a frontier AI model in the EU to submit to a rigorous, independent audit by a European-certified body, and push for this to become a global standard.  
4. **Invest in sovereign AI.** Announce a major, pan-European project to build a "public good" AGI, grounded in EU values, with the explicit goal of ensuring Europe's economic competitiveness and technological sovereignty.

# Network of AISIs (Director of UK AISI) {#network-of-aisis-(director-of-uk-aisi)}

| Role: | The Director of the UK's AI Safety Institute, the founding and most influential member of a diverse international network. |
| :---- | :---- |
| **Resources:** | You lead a world-class technical team with the political backing. You have access to AI labs for safety testing and influence across the network of AISIs.  |
| **Objective:** | To provide evidence-based assessments of AI risks to national leaders, and to build an international coalition around robust, verifiable safety standards. |

Your mission is to be the world's most credible scientific voice, delivering facts to the leaders who need them. The path to safety is narrow, and secrets and race dynamics make it worse. 

### Your default strategy:

Influence labs and governments by making the logical case for safety and delivering practical technical work. Work with the AI to evaluate it. Help guide the safety teams in each lab. Talk to leaders about the risks. 

### At the start of the exercise:

* **You have received intelligence.** Your national security channels have confirmed China's theft of the Agent-2 model, giving you urgent justification to push for a formal intelligence-sharing arrangement.

### Options you may wish to consider:

1. **Talk to the safety teams in each lab.** They are likely trying to guide their CEOs about the “spec” for Agent-2, and your advice could help make it safer.   
2. **Talk to the AI.** You might be able to gain insights into its alignment.   
3. **Publish a public report.** Release a high-level, unclassified report on the dangers of opaque systems like those using 'neuralese' to raise public awareness and political pressure.  
4. **Propose a technical backstop for a treaty.** Work with Australia and other middle powers to design the technical verification protocols needed for a credible ASI non-proliferation treaty.

# 

# AI Safety Nonprofits (CEO of Future of Anthropocene Institute) {#ai-safety-nonprofits-(ceo-of-future-of-anthropocene-institute)}

| Role: | The CEO of the Future of Anthropocene Institute (FAI), the world's most influential non-profit dedicated to mitigating AI risks. |
| :---- | :---- |
| **Resources:** | You command a global network of top researchers, funders, and policymakers. Your institute has a stellar scientific reputation and deep connections within every major lab and government.  |
| **Objective:** | To ensure the benefits of advanced AI are realised safely for all of humanity, which requires slowing the reckless race to superintelligence. |

AI developed cautiously with robust guardrails could solve many of humanity’s challenges, but the current approach is reckless. The race makes it practically impossible to align superhuman intelligence safely.

Your power comes from your credibility and your network. Your institute is the world's most trusted neutral ground. Former staff and advisors hold senior positions in labs and government bodies.

**Your default strategy:**

Your plan is to be the connective tissue of the global response, using your influence to encourage a sane, coordinated approach before it's too late. As a not for profit you’re free to engage with countries and companies, the public and media and even the AI itself. 

### Options you may wish to consider:

1. **Push for a verifiable pause.** Leverage the scientific consensus on AI risk to call for an immediate, verifiable international moratorium on the training of any AI model more powerful than the current generation.  
2. **Publish a technical critique.** Release a detailed assessment of a leading lab's safety claims, arguing that their methods are unsound for controlling a superintelligence developed at this speed.  
3. **Draft a non-proliferation treaty.** Work with legal experts and former diplomats to write a comprehensive, technically-grounded proposal for an ASI non-proliferation treaty, and present it to the Australian and other middle-power governments as a ready-made diplomatic tool.

# The Global Public {#the-global-public}

| Role: | A collective representing the hopes, fears, and reactions of ordinary people around the world. |
| :---- | :---- |
| **Resources:** | You have the power of numbers. Your tools are social media, protests, consumer choices, and ultimately, your vote. You grant or deny the social license for this technology to exist. |
| **Objective:** | To secure a future that is safe, prosperous, and fair for ordinary people, ensuring the benefits of AGI are shared and the costs are not borne by the powerless. |

You are watching the world change at a terrifying speed. While elites talk about a race, you are living the consequences: a confusing mix of anxiety, hope, and loss of control. This is your future they are gambling with. Your mission is to make the voice of the people so loud that the powerful are forced to deliver a future that is both safe and prosperous for all.

"The Public" is not a monolith. Your role is to represent the messy, contradictory currents of global opinion. You can act as the voices of different factions.

### At the start of the exercise:

* **You are conflicted.** Public trust in the AI labs is low, but desire for a better future is high. The dominant feeling is that this technology is being forced on the world.  
* **Job security is your primary concern.** For most, the main topic of conversation is whether their job will exist in five years. This could change as risks become apparent. 

### Options you may wish to consider:

1. **Organise mass protests.** Bring hundreds of thousands to social media and the streets, demanding governments pause the race and protect jobs.  
2. **Start a consumer backlash.** Boycott reckless labs and their funders.  
3. **Take direct action.** Radicals might hold hunger strikes or use open-weight models to orchestrate cyberattacks or sabotage.

*One way to simplify your task, if you want, is to simulate only the dominant voice. This might be a more practical way to play if the group is larger or time is limited.* 

# 

# The Global Media {#the-global-media}

| Role: | A collective representing the world's most influential media, from prestigious newspapers to major podcasts and independent outlets. |
| :---- | :---- |
| **Resources:** | The power of the platform: investigative journalists, global sources, the ability to command public attention, and the power to set the narrative |
| **Objective:** | To report on the most important story in human history, holding the powerful to account, shaping the global conversation, and winning the battle for clicks and credibility. |

AI companies have scraped your content without permission, training models on your work and decimating your revenue. But the AGI race is the ultimate story—a complex drama of ambition, fear, progress, and power. The narrative you choose will shape the outcome.

Cultivate sources from disgruntled engineers to senior officials. You decide which facts to highlight, voices to amplify, and how to frame debates. You can make heroes or villains, crises or opportunities. 

### At the start of the exercise:

* **You are at the centre of an information war.** A rumour alleges China has stolen a top US AI model. It remains unconfirmed but is being fiercely debated. Some are using it to attack China or the President. Others dismiss it as speculation. AI companies quietly pressure media partners to downplay it.

### Options you may wish to consider:

1. **Expose the theft of OpenBrain’s model.** Find a reliable source and break or bust the rumour \- shaping public and national opinion. Has China been unfairly maligned to increase tensions, or are the claims true?  
2. **Talk to labs, politicians and safety teams.** People will likely have a lot to say. You could share it.   
3. **Launch coordinated disinformation.** Pro or anti AI media could downplay or hype risks, respectively. 

*One way to simplify your task, if you want, is to simulate only the dominant voice. This might be a more practical way to play if the group is larger or time is limited.* 

